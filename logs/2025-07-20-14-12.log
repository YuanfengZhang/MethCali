Verbosity: 2 (Standard Logging)
=================== System Info ===================
AutoGluon Version:  1.3.1
Python Version:     3.10.18
Operating System:   Linux
Platform Machine:   x86_64
Platform Version:   #67~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 24 15:19:46 UTC 2
CPU Count:          160
Memory Avail:       954.90 GB / 1007.46 GB (94.8%)
Disk Space Avail:   1232.86 GB / 3519.75 GB (35.0%)
===================================================
Presets specified: ['experimental_quality']
Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)
Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1
DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.
	This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.
	Running DyStack for up to 21600s of the 86400s of remaining time (25%).
	Running DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.
		Context path: "/hot_warm_data/zhangyuanfeng/methylation/models/2025-07-20-14-12/ds_sub_fit/sub_fit_ho"
Leaderboard on holdout data (DyStack):
                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val      fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
0          LightGBM_BAG_L2      -6.239273  -6.348692  root_mean_squared_error      444.232194    1056.527377  16518.881374               153.185015              208.458032        2974.134225            2       True          9
1      WeightedEnsemble_L3      -6.251155  -6.341936  root_mean_squared_error      540.287528    1211.103880  19204.028462                 0.018243                0.185939           1.548219            3       True         11
2        LightGBMXT_BAG_L2      -6.303276  -6.375989  root_mean_squared_error      387.084270    1002.459909  16228.346019                96.037091              154.390564        2683.598869            2       True          8
3      WeightedEnsemble_L2      -6.655146  -6.688445  root_mean_squared_error      170.844818     216.016015   8688.903960                 0.014959                0.171294           1.150812            2       True          7
4          CatBoost_BAG_L1      -6.655214  -6.690033  root_mean_squared_error       19.832027       8.414834   6152.390304                19.832027                8.414834        6152.390304            1       True          4
5          CatBoost_BAG_L2      -6.750058  -6.786115  root_mean_squared_error      293.979147     852.736486  14410.053407                 2.931968                4.667141         865.306257            2       True         10
6          LightGBM_BAG_L1      -7.152767  -7.208238  root_mean_squared_error      150.997832     207.429886   2535.362844               150.997832              207.429886        2535.362844            1       True          2
7        LightGBMXT_BAG_L1      -7.227339  -7.260493  root_mean_squared_error       97.015893     152.364835   2499.338423                97.015893              152.364835        2499.338423            1       True          1
8           XGBoost_BAG_L1      -7.320153  -7.343133  root_mean_squared_error       19.922518      34.067607    758.615633                19.922518               34.067607         758.615633            1       True          6
9   RandomForestMSE_BAG_L1      -7.398339  -7.426758  root_mean_squared_error        2.298765     349.082534   1270.158289                 2.298765              349.082534        1270.158289            1       True          3
10    ExtraTreesMSE_BAG_L1      -7.503839  -7.531523  root_mean_squared_error        0.980145      96.709649    328.881656                 0.980145               96.709649         328.881656            1       True          5
	1	 = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)
	22184s	 = DyStack   runtime |	64216s	 = Remaining runtime
Starting main fit with num_stack_levels=1.
	For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`
Beginning AutoGluon training ... Time limit = 64216s
AutoGluon will save models to "/hot_warm_data/zhangyuanfeng/methylation/models/2025-07-20-14-12"
Train Data Rows:    12024340
Train Data Columns: 45
Label Column:       actual_beta
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    973243.50 MB
	Train Data (Original)  Memory Usage: 6522.37 MB (0.7% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
			Note: Converting 4 features to boolean dtype as they only contain 2 unique values.
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
		Fitting CategoryFeatureGenerator...
			Fitting CategoryMemoryMinimizeFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Stage 5 Generators:
		Fitting DropDuplicatesFeatureGenerator...
	Useless Original Features (Count: 1): ['beta_min']
		These features carry no predictive signal and should be manually investigated.
		This is typically a feature which has the same value for all rows.
		These features do not need to be present at inference time.
	Types of features in original data (raw dtype, special dtypes):
		('bool', [])   :  2 | ['enhancer', 'promoter']
		('float', [])  : 36 | ['GC%_70', 'CpG_GC_ratio_70', 'GC_skew_70', 'ShannonEntropy_70', 'BWT_ratio_70', ...]
		('int', [])    :  2 | ['depth', 'depth_peak_num']
		('object', []) :  4 | ['seq_5', 'cpg', 'location', 'method']
	Types of features in processed data (raw dtype, special dtypes):
		('category', [])  :  4 | ['seq_5', 'cpg', 'location', 'method']
		('float', [])     : 34 | ['GC%_70', 'CpG_GC_ratio_70', 'GC_skew_70', 'ShannonEntropy_70', 'BWT_ratio_70', ...]
		('int', [])       :  2 | ['depth', 'depth_peak_num']
		('int', ['bool']) :  4 | ['enhancer', 'promoter', 'beta_q5', 'beta_q90']
	52.8s = Fit runtime
	44 features in original data used to generate 44 features in processed data.
	Train Data (Processed) Memory Usage: 3382.86 MB (0.3% of available memory)
Data preprocessing and feature engineering runtime = 57.89s ...
AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	To change this, specify the eval_metric parameter of Predictor()
Large model count detected (119 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.
User-specified model hyperparameters to be fit:
{
	'TABPFNMIX': [{'model_path_classifier': 'autogluon/tabpfn-mix-1.0-classifier', 'model_path_regressor': 'autogluon/tabpfn-mix-1.0-regressor', 'n_ensembles': 1, 'max_epochs': 30, 'ag.sample_rows_val': 5000, 'ag.max_rows': 50000, 'ag_args': {'name_suffix': '_v1'}}],
	'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],
	'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],
	'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],
	'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],
	'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],
	'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
	'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
	'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],
}
AutoGluon will fit 2 stack levels (L1 to L2) ...
Excluded models: ['KNN'] (Specified by `excluded_model_types`)
Fitting 107 L1 models, fit_strategy="sequential" ...
Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 42761.67s of the 64158.54s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.20%)
	-7.2549	 = Validation score   (-root_mean_squared_error)
	2471.43s	 = Training   runtime
	176.5s	 = Validation runtime
Fitting model: LightGBM_BAG_L1 ... Training model for up to 40240.54s of the 61637.41s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.20%)
	-7.1982	 = Validation score   (-root_mean_squared_error)
	2686.55s	 = Training   runtime
	242.93s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 37482.82s of the 58879.69s of remaining time.
	-7.42	 = Validation score   (-root_mean_squared_error)
	1447.2s	 = Training   runtime
	377.26s	 = Validation runtime
Fitting model: CatBoost_BAG_L1 ... Training model for up to 35652.75s of the 57049.62s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.24%)
	-6.111	 = Validation score   (-root_mean_squared_error)
	28918.85s	 = Training   runtime
	24.99s	 = Validation runtime
Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6628.33s of the 28025.20s of remaining time.
	-7.52	 = Validation score   (-root_mean_squared_error)
	1008.24s	 = Training   runtime
	365.02s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5249.40s of the 26646.27s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=3.87%)
	Time limit exceeded... Skipping NeuralNetFastAI_BAG_L1.
Fitting model: TabPFNMix_v1_BAG_L1 ... Training model for up to 4623.92s of the 26020.79s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.99%)
	Warning: Exception caused TabPFNMix_v1_BAG_L1 to fail during training... Skipping this model.
		[36mray::_ray_fit()[39m (pid=381171, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9619472)
Detailed Traceback:
Traceback (most recent call last):
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2169, in _train_and_save
    model = self._train_single(**model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2055, in _train_single
    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py", line 270, in _fit
    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 390, in _fit
    self._fit_folds(
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 848, in _fit_folds
    fold_fitting_strategy.after_all_folds_scheduled()
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 690, in after_all_folds_scheduled
    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 631, in _run_parallel
    self._process_fold_results(finished, unfinished, fold_ctx)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 587, in _process_fold_results
    raise processed_exception
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 550, in _process_fold_results
    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 2782, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 929, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AssertionError): [36mray::_ray_fit()[39m (pid=381171, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9619472)
Fitting model: XGBoost_BAG_L1 ... Training model for up to 4606.52s of the 26003.38s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.91%)
	-7.2769	 = Validation score   (-root_mean_squared_error)
	3768.66s	 = Training   runtime
	90.16s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 809.13s of the 22206.00s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.12%)
	Time limit exceeded... Skipping NeuralNetTorch_BAG_L1.
Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 699.11s of the 22095.98s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.21%)
	-7.2491	 = Validation score   (-root_mean_squared_error)
	675.16s	 = Training   runtime
	58.19s	 = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 4276.17s of the 21388.95s of remaining time.
	Ensemble Weights: {'CatBoost_BAG_L1': 1.0}
	-6.111	 = Validation score   (-root_mean_squared_error)
	1.31s	 = Training   runtime
	0.19s	 = Validation runtime
Excluded models: ['KNN'] (Specified by `excluded_model_types`)
Fitting 107 L2 models, fit_strategy="sequential" ...
Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 21386.87s of the 21385.20s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.42%)
	-5.7432	 = Validation score   (-root_mean_squared_error)
	2880.75s	 = Training   runtime
	179.63s	 = Validation runtime
Fitting model: LightGBM_BAG_L2 ... Training model for up to 18436.76s of the 18435.10s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.46%)
	-5.7107	 = Validation score   (-root_mean_squared_error)
	3160.85s	 = Training   runtime
	241.12s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 15179.86s of the 15178.19s of remaining time.
	Warning: Reducing model 'n_estimators' from 300 -> 172 due to low time. Expected time usage reduced from 26369.8s -> 15152.5s...
	-5.7828	 = Validation score   (-root_mean_squared_error)
	1623.92s	 = Training   runtime
	254.27s	 = Validation runtime
Fitting model: CatBoost_BAG_L2 ... Training model for up to 13286.64s of the 13284.97s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.51%)
	-5.797	 = Validation score   (-root_mean_squared_error)
	10703.79s	 = Training   runtime
	9.8s	 = Validation runtime
Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 2537.46s of the 2535.79s of remaining time.
	Warning: Reducing model 'n_estimators' from 300 -> 113 due to low time. Expected time usage reduced from 6656.2s -> 2512.3s...
	-5.792	 = Validation score   (-root_mean_squared_error)
	635.61s	 = Training   runtime
	163.35s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1725.00s of the 1723.33s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=4.29%)
	Time limit exceeded... Skipping NeuralNetFastAI_BAG_L2.
Fitting model: TabPFNMix_v1_BAG_L2 ... Training model for up to 1625.17s of the 1623.51s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=3.23%)
	Warning: Exception caused TabPFNMix_v1_BAG_L2 to fail during training... Skipping this model.
		[36mray::_ray_fit()[39m (pid=943406, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9619472)
Detailed Traceback:
Traceback (most recent call last):
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2169, in _train_and_save
    model = self._train_single(**model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2055, in _train_single
    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py", line 270, in _fit
    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 390, in _fit
    self._fit_folds(
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 848, in _fit_folds
    fold_fitting_strategy.after_all_folds_scheduled()
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 690, in after_all_folds_scheduled
    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 631, in _run_parallel
    self._process_fold_results(finished, unfinished, fold_ctx)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 587, in _process_fold_results
    raise processed_exception
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 550, in _process_fold_results
    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 2782, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 929, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AssertionError): [36mray::_ray_fit()[39m (pid=943406, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9619472)
Fitting model: XGBoost_BAG_L2 ... Training model for up to 1583.16s of the 1581.49s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=3.22%)
	-5.7552	 = Validation score   (-root_mean_squared_error)
	1301.13s	 = Training   runtime
	46.06s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 243.09s of the 241.42s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.36%)
	Time limit exceeded... Skipping NeuralNetTorch_BAG_L2.
Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 81.37s of the 79.70s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.47%)
	Time limit exceeded... Skipping LightGBMLarge_BAG_L2.
Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 40.02s of the 38.35s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.51%)
	Time limit exceeded... Skipping CatBoost_r177_BAG_L2.
Fitting model: WeightedEnsemble_L3 ... Training model for up to 2138.69s of the -5.26s of remaining time.
	Ensemble Weights: {'LightGBM_BAG_L2': 0.667, 'LightGBMXT_BAG_L2': 0.167, 'CatBoost_BAG_L2': 0.125, 'RandomForestMSE_BAG_L2': 0.042}
	-5.7016	 = Validation score   (-root_mean_squared_error)
	2.23s	 = Training   runtime
	0.21s	 = Validation runtime
AutoGluon training complete, total runtime = 64228.92s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1966.9 rows/s (2404868 batch size)
TabularPredictor saved. To load, use: predictor = TabularPredictor.load("/hot_warm_data/zhangyuanfeng/methylation/models/2025-07-20-14-12")
These features in provided data are not utilized by the predictor and will be ignored: ['beta_min']
Computing feature importance via permutation shuffling for 44 features using 5000 rows with 5 shuffle sets...
	40017.13s	= Expected runtime (8003.43s per shuffle set)
	2229.79s	= Actual runtime (Completed 5 of 5 shuffle sets)
