Verbosity: 2 (Standard Logging)
=================== System Info ===================
AutoGluon Version:  1.3.1
Python Version:     3.10.18
Operating System:   Linux
Platform Machine:   x86_64
Platform Version:   #67~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 24 15:19:46 UTC 2
CPU Count:          160
Memory Avail:       950.67 GB / 1007.46 GB (94.4%)
Disk Space Avail:   1000.61 GB / 3519.75 GB (28.4%)
===================================================
Presets specified: ['experimental_quality']
Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)
Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1
DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.
	This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.
	Running DyStack for up to 21600s of the 86400s of remaining time (25%).
	Running DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.
		Context path: "/hot_warm_data/zhangyuanfeng/methylation/models/2025-07-22-18-53/ds_sub_fit/sub_fit_ho"
Leaderboard on holdout data (DyStack):
                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val      fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
0          LightGBM_BAG_L2      -6.227125  -6.313807  root_mean_squared_error      435.076563    1049.442421  16231.627695               144.888694              197.215935        2693.136783            2       True          9
1      WeightedEnsemble_L3      -6.238661  -6.307386  root_mean_squared_error      527.516409    1194.560995  18739.945477                 0.021362                0.171649           1.420031            3       True         12
2        LightGBMXT_BAG_L2      -6.292934  -6.343830  root_mean_squared_error      382.606354     997.173411  16045.388662                92.418484              144.946924        2506.897751            2       True          8
3          CatBoost_BAG_L2      -6.558485  -6.569526  root_mean_squared_error      292.624614     856.427435  14783.852292                 2.436744                4.200948        1245.361380            2       True         10
4          CatBoost_BAG_L1      -6.652699  -6.662091  root_mean_squared_error       22.911803       8.906000   6244.561677                22.911803                8.906000        6244.561677            1       True          4
5      WeightedEnsemble_L2      -6.653233  -6.661202  root_mean_squared_error      170.345315     210.681212   8685.919618                 0.016572                0.173527           1.256421            2       True          7
6          LightGBM_BAG_L1      -7.175511  -7.205097  root_mean_squared_error      147.416940     201.601684   2440.101519               147.416940              201.601684        2440.101519            1       True          2
7        LightGBMXT_BAG_L1      -7.251748  -7.257039  root_mean_squared_error       95.168813     152.258933   2359.964715                95.168813              152.258933        2359.964715            1       True          1
8           XGBoost_BAG_L1      -7.339848  -7.333989  root_mean_squared_error       21.322103      34.608757    767.618690                21.322103               34.608757         767.618690            1       True          6
9   RandomForestMSE_BAG_L1      -7.429957  -7.422725  root_mean_squared_error        2.352787     352.400035   1402.776089                 2.352787              352.400035        1402.776089            1       True          3
10    ExtraTreesMSE_BAG_L1      -7.531524  -7.525336  root_mean_squared_error        1.015425     102.451077    323.468221                 1.015425              102.451077         323.468221            1       True          5
11          XGBoost_BAG_L2     -24.812425 -24.814661  root_mean_squared_error      310.178785     883.701878  13886.599522                19.990915               31.475391         348.108611            2       True         11
	1	 = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)
	22478s	 = DyStack   runtime |	63922s	 = Remaining runtime
Starting main fit with num_stack_levels=1.
	For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`
Beginning AutoGluon training ... Time limit = 63922s
AutoGluon will save models to "/hot_warm_data/zhangyuanfeng/methylation/models/2025-07-22-18-53"
Train Data Rows:    12020898
Train Data Columns: 45
Label Column:       actual_beta
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    985406.60 MB
	Train Data (Original)  Memory Usage: 6518.83 MB (0.7% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
			Note: Converting 4 features to boolean dtype as they only contain 2 unique values.
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
		Fitting CategoryFeatureGenerator...
			Fitting CategoryMemoryMinimizeFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Stage 5 Generators:
		Fitting DropDuplicatesFeatureGenerator...
	Useless Original Features (Count: 1): ['beta_min']
		These features carry no predictive signal and should be manually investigated.
		This is typically a feature which has the same value for all rows.
		These features do not need to be present at inference time.
	Types of features in original data (raw dtype, special dtypes):
		('bool', [])   :  2 | ['enhancer', 'promoter']
		('float', [])  : 36 | ['GC%_70', 'CpG_GC_ratio_70', 'GC_skew_70', 'ShannonEntropy_70', 'BWT_ratio_70', ...]
		('int', [])    :  2 | ['depth', 'depth_peak_num']
		('object', []) :  4 | ['seq_5', 'cpg', 'location', 'method']
	Types of features in processed data (raw dtype, special dtypes):
		('category', [])  :  4 | ['seq_5', 'cpg', 'location', 'method']
		('float', [])     : 34 | ['GC%_70', 'CpG_GC_ratio_70', 'GC_skew_70', 'ShannonEntropy_70', 'BWT_ratio_70', ...]
		('int', [])       :  2 | ['depth', 'depth_peak_num']
		('int', ['bool']) :  4 | ['enhancer', 'promoter', 'beta_q5', 'beta_q90']
	46.6s = Fit runtime
	44 features in original data used to generate 44 features in processed data.
	Train Data (Processed) Memory Usage: 3381.89 MB (0.3% of available memory)
Data preprocessing and feature engineering runtime = 51.72s ...
AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	To change this, specify the eval_metric parameter of Predictor()
Large model count detected (119 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.
User-specified model hyperparameters to be fit:
{
	'TABPFNMIX': [{'model_path_classifier': 'autogluon/tabpfn-mix-1.0-classifier', 'model_path_regressor': 'autogluon/tabpfn-mix-1.0-regressor', 'n_ensembles': 1, 'max_epochs': 30, 'ag.sample_rows_val': 5000, 'ag.max_rows': 50000, 'ag_args': {'name_suffix': '_v1'}}],
	'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],
	'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],
	'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],
	'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],
	'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],
	'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
	'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
	'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],
}
AutoGluon will fit 2 stack levels (L1 to L2) ...
Excluded models: ['KNN'] (Specified by `excluded_model_types`)
Fitting 107 L1 models, fit_strategy="sequential" ...
Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 42569.30s of the 63869.91s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.17%)
	-7.2541	 = Validation score   (-root_mean_squared_error)
	2409.58s	 = Training   runtime
	167.67s	 = Validation runtime
Fitting model: LightGBM_BAG_L1 ... Training model for up to 40112.63s of the 61413.24s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.17%)
	-7.1981	 = Validation score   (-root_mean_squared_error)
	2496.43s	 = Training   runtime
	217.31s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 37547.27s of the 58847.88s of remaining time.
	-7.4195	 = Validation score   (-root_mean_squared_error)
	1270.89s	 = Training   runtime
	353.4s	 = Validation runtime
Fitting model: CatBoost_BAG_L1 ... Training model for up to 35917.60s of the 57218.21s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.21%)
	-6.1428	 = Validation score   (-root_mean_squared_error)
	29183.41s	 = Training   runtime
	38.67s	 = Validation runtime
Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6663.07s of the 27963.68s of remaining time.
	-7.519	 = Validation score   (-root_mean_squared_error)
	1001.05s	 = Training   runtime
	365.95s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5290.37s of the 26590.98s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=3.93%)
	-8.1999	 = Validation score   (-root_mean_squared_error)
	2522.27s	 = Training   runtime
	80.21s	 = Validation runtime
Fitting model: TabPFNMix_v1_BAG_L1 ... Training model for up to 2739.73s of the 24040.34s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.98%)
	Warning: Exception caused TabPFNMix_v1_BAG_L1 to fail during training... Skipping this model.
		[36mray::_ray_fit()[39m (pid=3040798, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9616718)
Detailed Traceback:
Traceback (most recent call last):
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2169, in _train_and_save
    model = self._train_single(**model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2055, in _train_single
    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py", line 270, in _fit
    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 390, in _fit
    self._fit_folds(
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 848, in _fit_folds
    fold_fitting_strategy.after_all_folds_scheduled()
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 690, in after_all_folds_scheduled
    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 631, in _run_parallel
    self._process_fold_results(finished, unfinished, fold_ctx)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 587, in _process_fold_results
    raise processed_exception
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 550, in _process_fold_results
    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 2782, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 929, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AssertionError): [36mray::_ray_fit()[39m (pid=3040798, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9616718)
Fitting model: XGBoost_BAG_L1 ... Training model for up to 2720.78s of the 24021.39s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.91%)
	-7.2984	 = Validation score   (-root_mean_squared_error)
	2237.78s	 = Training   runtime
	63.24s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 457.88s of the 21758.49s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.14%)
	Time limit exceeded... Skipping NeuralNetTorch_BAG_L1.
Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 353.33s of the 21653.94s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.24%)
	-7.2863	 = Validation score   (-root_mean_squared_error)
	325.46s	 = Training   runtime
	13.4s	 = Validation runtime
Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 12.14s of the 21312.75s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.25%)
	Time limit exceeded... Skipping CatBoost_r177_BAG_L1.
Fitting model: WeightedEnsemble_L2 ... Training model for up to 4256.93s of the 21294.71s of remaining time.
	Ensemble Weights: {'CatBoost_BAG_L1': 1.0}
	-6.1428	 = Validation score   (-root_mean_squared_error)
	1.5s	 = Training   runtime
	0.21s	 = Validation runtime
Excluded models: ['KNN'] (Specified by `excluded_model_types`)
Fitting 107 L2 models, fit_strategy="sequential" ...
Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 21292.32s of the 21290.27s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.46%)
	-5.7874	 = Validation score   (-root_mean_squared_error)
	3089.33s	 = Training   runtime
	174.48s	 = Validation runtime
Fitting model: LightGBM_BAG_L2 ... Training model for up to 18134.88s of the 18132.83s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.51%)
	-5.7563	 = Validation score   (-root_mean_squared_error)
	3438.19s	 = Training   runtime
	229.16s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 14606.31s of the 14604.26s of remaining time.
	Warning: Reducing model 'n_estimators' from 300 -> 167 due to low time. Expected time usage reduced from 26096.0s -> 14578.5s...
	-5.8224	 = Validation score   (-root_mean_squared_error)
	1684.46s	 = Training   runtime
	270.56s	 = Validation runtime
Fitting model: CatBoost_BAG_L2 ... Training model for up to 12636.15s of the 12634.11s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.52%)
	-5.8395	 = Validation score   (-root_mean_squared_error)
	10166.66s	 = Training   runtime
	9.59s	 = Validation runtime
Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 2421.40s of the 2419.35s of remaining time.
	Warning: Reducing model 'n_estimators' from 300 -> 98 due to low time. Expected time usage reduced from 7277.7s -> 2391.8s...
	-5.8302	 = Validation score   (-root_mean_squared_error)
	507.23s	 = Training   runtime
	146.21s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1752.39s of the 1750.34s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=4.29%)
	Time limit exceeded... Skipping NeuralNetFastAI_BAG_L2.
Fitting model: TabPFNMix_v1_BAG_L2 ... Training model for up to 1647.38s of the 1645.33s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=3.22%)
	Warning: Exception caused TabPFNMix_v1_BAG_L2 to fail during training... Skipping this model.
		[36mray::_ray_fit()[39m (pid=3390109, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9616718)
Detailed Traceback:
Traceback (most recent call last):
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2169, in _train_and_save
    model = self._train_single(**model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py", line 2055, in _train_single
    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py", line 270, in _fit
    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 390, in _fit
    self._fit_folds(
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py", line 848, in _fit_folds
    fold_fitting_strategy.after_all_folds_scheduled()
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 690, in after_all_folds_scheduled
    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 631, in _run_parallel
    self._process_fold_results(finished, unfinished, fold_ctx)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 587, in _process_fold_results
    raise processed_exception
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 550, in _process_fold_results
    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 2782, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/ray/_private/worker.py", line 929, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AssertionError): [36mray::_ray_fit()[39m (pid=3390109, ip=192.168.3.9)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py", line 413, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py", line 1051, in fit
    out = self._fit(**kwargs)
  File "/home/zhangyuanfeng/mambaforge/envs/ag_cpu/lib/python3.10/site-packages/autogluon/tabular/models/tabpfnmix/tabpfnmix_model.py", line 128, in _fit
    raise AssertionError(f"Skipping model due to X having more rows than `ag.max_rows={max_rows}` (len(X)={len(X)})")
AssertionError: Skipping model due to X having more rows than `ag.max_rows=50000` (len(X)=9616718)
Fitting model: XGBoost_BAG_L2 ... Training model for up to 1603.13s of the 1601.08s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=3.23%)
	-5.7972	 = Validation score   (-root_mean_squared_error)
	1317.83s	 = Training   runtime
	46.44s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 246.13s of the 244.08s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.32%)
	Time limit exceeded... Skipping NeuralNetTorch_BAG_L2.
Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 96.46s of the 94.41s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.42%)
	Time limit exceeded... Skipping LightGBMLarge_BAG_L2.
Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 53.78s of the 51.73s of remaining time.
	Fitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=120, gpus=0, memory=2.46%)
	Time limit exceeded... Skipping CatBoost_r177_BAG_L2.
Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 10.80s of the 8.75s of remaining time.
	Warning: Model has no time left to train, skipping model... (Time Left = -2.7s)
	Time limit exceeded... Skipping NeuralNetTorch_r79_BAG_L2.
Fitting model: WeightedEnsemble_L3 ... Training model for up to 2129.23s of the -5.99s of remaining time.
	Ensemble Weights: {'LightGBM_BAG_L2': 0.64, 'LightGBMXT_BAG_L2': 0.16, 'CatBoost_BAG_L2': 0.12, 'RandomForestMSE_BAG_L2': 0.04, 'XGBoost_BAG_L2': 0.04}
	-5.7483	 = Validation score   (-root_mean_squared_error)
	1.94s	 = Training   runtime
	0.18s	 = Validation runtime
AutoGluon training complete, total runtime = 63934.57s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1941.7 rows/s (2404180 batch size)
TabularPredictor saved. To load, use: predictor = TabularPredictor.load("/hot_warm_data/zhangyuanfeng/methylation/models/2025-07-22-18-53")
These features in provided data are not utilized by the predictor and will be ignored: ['beta_min']
Computing feature importance via permutation shuffling for 44 features using 5000 rows with 5 shuffle sets...
	38084.49s	= Expected runtime (7616.9s per shuffle set)
	2104.89s	= Actual runtime (Completed 5 of 5 shuffle sets)
